---
layout: default
title: "Bridging the Knowledge-doing gap: a study of in-context demonstrations for LLM agents"
preview_image: /assets/sarl/sarl_cover.png
date: 2025-09-08
permalink: /projects/sarl_project/
---

# BRIDGING THE KNOWLEDGE–DOING GAP: A STUDYOF IN-CONTEXT DEMONSTRATIONS FOR LLM AGENTS

> **Full report:** [Download PDF](/assets/sarl/bridging_knowledge_doing.pdf)

# Introduction

Large Language Models (LLMs) excel at reasoning, insturciton following, and factual recall, however they consistently struggle when deployed as autnomous agents in interactive environments. This mismatch, known as the **Knowledge-doing gap**, arises when models can aritculate correct strategies but fail to execute them reliably over long deciison horizons that require planning , exploration and credit assignment.

An approach to mitigate this gap is **in-context learning**, where LLMs adapt at inference time by conditioning on task-relevant context such as expert demonstrations or past trajectories. In interactive settings, this idea extends to **in-context reinforcement learning** where models observe sequences of states, actions and rewards. While demonstrations can range from optinal to sub-optimal to encourage exploration, recent work shos that naively increasing the number of demonstrations often leads to diminshing returns due to contextual interference. 

The **LMAct** framework evalutes this phnomenon in controlled sequential decision-making tasks. Results show that LLMs can imitate demonstrated behavoir, but scaling demonstrations does not reliably improve generalization and can even undermine performance. To study these limitations in more complex settings, use the **BALROG** benchmark that evaluates LLM agents in long-horizon game environments. BALROG exposes persisten failures as task complexity and planning horizon increase, showing that knowledge/doing gap remians largley unresolved.

Building on these insights, this project evlautes how dmeonstrations and contextual information influence LLM agnet performance across both LMAct and BLAROG. Our expeirments show that **data structure and ordering matter mora tan raw data quality** and that excessive or reduntant context harms performance. While added context can help in simpler enviornments, it does not scale robustly to complex ones. 

Motivated by these findings, we shift focus from *how many demonstrations are provided to how they are selected. We propose faming demonstration selection as a sequential decision-makingp problem , modeled as a finite-horizon Markov Decision Process. An external RL policy adaptively selects divdrse, relevant dmeonstrations while keeping the LLM fixed, aiming to reduce interference and improve task performance. This formulation guides ongoing work toward apative, leanring-based context contruction rather than naive demosntration accumulation.


# Methodology

This proejct evalutes LLMs as decision-making agents using two experimentala setups.
First we employ **LMAct** to study in-context demonstartions and prompt-level conditioning in a contrlled, low-complexity sequential decision-making environment. This setting allows us to isolate how demonstration, ordering, and quantity affect agent behavior.

We then extend the analysis to **BALROG** a standardized benchmark designed to evalute agentic behavior in **long-horizon, procedurally generated environments**. BALROG introduces higher complexity, stochasticity, and exploration demands, making it well.suited for exposing failure modes that are not visible in short-episode tasks.

Togethere, we provide an overview for assesing how contextual infomration influences performance as task difficulty increases. Based on the empiricla limitations observed in both LMAct and BALROG, we further propose a a principled formualtion of demonstration selection as a sequentila decision-making problme, modelede as a finite-horizoin Markov Decision Process taht explicitly tracks diverssity among selected demonstrations.

Both methodologies are illustraed in **Figure 1**



![Project diagram](/assets/sarl/project_diagram.png)
*Figure 1. Project methodology evaluating LLM agent on LMAct and BALROG frameworks.*

## LMAct Methodology


To analyze the effect onf in-context demonstrations on sequential decision-making, we use the **LMAct framework** with **GPT-4o** as chosen LLM. Exeriments are conducted in the **ASCII Tic-Tac-Toe** environment introduced in the original LMAct work, which provides a controlled setting for studying agent behavior.

At each decision step, the model is prompted with a structured context consisting of four components:
- **Current game state**  
  An ASCII 3×3 Tic-Tac-Toe board representing the current configuration of the game.

- **Trajectory so far**  
  A history of all previous *(state, action)* pairs from the current episode.

- **Expert demonstration episodes**  
  Complete Tic-Tac-Toe trajectories generated by an expert policy. The number of demonstrations is varied between **21 and 28** to study demonstration scaling effects.

- **Instruction prompt**  
  A fixed instruction directing the model to select the next action based on the provided context.

Expert demonstrations are sampled uniformly at random from a pre-collected pool provided by the LMAct benchmark. Each experimental configuration is evaluated over **100 episodes**, with a maximum of **100 steps per episode**, and results are reported as the average final score across runs.

This setup enables a controlled analysis of how demonstration quantity, structure, and ordering influence in-context decision-making.


## BALROG Methodology 

To evaluate LLMs in more complex, long-horizon settings, we use the **:contentReference[oaicite:0]{index=0}** benchmark, a reinforcement-learning-inspired testbed designed to assess agentic behavior in procedurally generated video game environments. BALROG enables systematic evaluation of planning, reasoning, adaptability, and in-context learning under increasing task complexity.

In this project, we focus exclusively on **language-only models**, excluding vision–language models, in order to isolate the effects of reasoning, memory, and contextual conditioning on agent performance.

We evaluate agents across three BALROG environments with progressively increasing difficulty:

- **:contentReference[oaicite:1]{index=1}**  
  A short-horizon, low-variance environment with structured objectives, representing the simplest setting in our evaluation.

- **:contentReference[oaicite:2]{index=2}**  
  A highly stochastic, long-horizon survival environment that requires sustained planning and typically benefits from extensive experience.

- **:contentReference[oaicite:3]{index=3}**  
  A challenging environment with sparse rewards, medium stochasticity, and increased linguistic and combinatorial complexity.

Together, these environments allow for a comparative analysis of LLM-based agents across a spectrum of task horizons and difficulty levels.

All results are obtained following a standardized evaluation protocol. Agents are evaluated over **five random seeds**, with a fixed number of episodes per seed and **environment-specific step limits** chosen to match the expected task horizon. Performance is reported as **average game progress**, aggregated across all episodes and seeds.

| Environment | # Seeds | Max steps | # Episodes / seed | Total episodes | Notes |
|------------|---------|-----------|-------------------|----------------|-------|
| BabyAI     | 5       | 50        | 20                | 100            | Short horizon, low variance |
| Crafter    | 5       | 1000      | 20                | 100            | Highly stochastic, long-horizon survival; requires more trajectories |
| TextWorld  | 5       | 100       | 50                | 250            | Medium stochasticity |





## 5.1 LMAct Experiments  
### 5.1.1 In-Context Reinforcement Learning (ICRL) Prompting

We begin by establishing a baseline for **in-context reinforcement learning (ICRL)**. The hypothesis is that  **explicit reward feedback alone may be sufficient for an LLM to adapt its behavior purely in-context**, without any parameter updates, inspired by the *Reward Is Enough* line of work.

We extend the standard LMAct prompt by adding a **previous episode summary** component. In addition to the current game state, trajectory so far, expert demonstrations, and action instruction, the model now receives a compact summary of its most recent episode:

- **Summarized trajectory**: the first, middle, and last *(state, action, reward)* tuples from the previous episode, along with the final scalar reward obtained.

This summary provides  feedback about the overall behavior and outcome of the model’s last rollout. For the first episode, no summary is included.
All experiments follow the original LMAct setup across varying numbers of expert demonstrations, with the key difference that **scalar reward feedback is explicitly included**, unlike in the original LMAct study where rewards were omitted from the prompt. This allows us to directly assess whether reward-conditioned context enables in-context behavioral adaptation.

### 5.1.2 In-Context Reinforcement Learning (ICRL) Prompting

We replace raw expect trajectories with **single distilled natural-language heuristic**, imnspired by the **Reflexion** framework. Instread of providing multiple demos directly, a separate *Policy Analyst* LLM analyzes the expert trajectories and summarizes the expert's implicit strategy into a generalizable rule. 

This heuristic captures **what the expert prioritizes, how goals are achieved, and which behaviors are favored**, it is then provided to the acting LLM in place of the original demonstrations. The goal is to test whether **verbalized strategic guidance** can be more effective and less prone to contextual interference than long demonstration traces.

The experiment is conducted across **21 to 28 expert episodes**, with eachs et distilled into a single heuristic , enabling a direct comparison between raw trajectory conditioning and reflfexion style policy abstraction.



### 5.1.3 Contrastive Heuristic Generation

This experiment generates **one heuristic per expert trajectory** rather than a single aggregated rule. Each heuristic is accompanied by a **positive action example** that follows it and a **negative example** that violates it, providing contrastive grounding.

The goal is to supply the LLM with a diverse set of clearly grounded heuristics that cover different game scenarios, improving action selection and generalization through contrastive prompting.

### 5.1.4 Horizon-Curriculum Learning


This experiment focuses on **data structure rather than data type** by organizing expert demonstrations into a **curriculum based on trajectory length**, inspired by curriculum reinforcement learning.

Expert trajectories are grouped into three game-phase scenarios:

- **End-game scenario**  
  Short trajectories close to terminal states, emphasizing immediate wins and optimal final moves.

- **Mid-game scenario**  
  Medium-length trajectories that require multi-step reasoning and learning core heuristics to gain advantage.

- **Opening-game scenario**  
  Full-length games that illustrate how early moves lead into favorable mid-game positions.

Trajectories are assigned to each category based on their relative length within the sampled set. This structured ordering aims to help the LLM identify its current game phase and select appropriate actions, reducing contextual interference.



## 5.2 BALROG Experiments  
### 5.2.1 Short-Term Memory + Reasoning (CoT)



This experiment tests whether **short-term memory and reasoning traces** improve zero-shot performance in BALROG environments. A naive BALROG agent is equipped with limited inference-time memory, conditioning each action on recent observation–action history and a small number of chain-of-thought traces.

No demonstrations or task-specific examples are provided. Any performance gains arise solely from **reusing past interactions and reasoning at inference time**, isolating the effect of short-term memory as a baseline for long-horizon planning and exploration.


### 5.2.2 In-Context Learning (Few-Shot Imitation)

This experiment evaluates **few-shot in-context imitation** in BALROG by conditioning the agent on a small set of expert demonstrations. The agent reuses demonstrations across episodes via context caching and combines them with limited short-term reasoning memory.

By pairing expert examples with recent interaction history and chain-of-thought recall, this setup tests whether **in-context imitation can improve long-horizon planning beyond zero-shot behavior**, without updating model parameters.

### 5.2.3 Episodic Memory + In-Context Learning

This experiment combines **short-term reasoning memory** with **long-term episodic memory** in BALROG. Within each episode, the agent retains recent interactions and reasoning traces to support local planning. Across episodes, it caches its own past trajectories and reuses them as in-context demonstrations.

By conditioning on **self-generated experience rather than only expert demonstrations**, this setup approximates inference-time learning without parameter updates and evaluates whether structured memory alone can improve long-horizon planning.

## Results

### LMAct Results


![LMAct results 1](/assets/sarl/exp_1.png)
*Figure 1. Average scores of GPT-4o using standard, raw ex-pert demonstration episodes vs. the ’Previous episode summary’ component.*

![LMAct results 2](/assets/sarl/exp_2.png)
*Figure # Average scores of GPT-4o using standard demonstrations vs. compressing them into a single heuristic.*

![LMAct results 3](/assets/sarl/exp_3.png)
*Figure # Average scores of GPT-4o using standard, raw expert demonstration episodes vs. Average scores of GPT-4o by compressing N raw, expert demonstration episodes into N contrastive heuristics. Hence, the x-axis provides the number of raw expert demonstration episodes used for the baseline, and analogously provides the number of raw expert demonstration episodes used to create the same number of contrastive heuristics.*

![LMAct results 4](/assets/sarl/exp_4.png)
*Figure # Average scores of GPT-4o using standard, raw expert demonstration episodes vs. Average scores of GPT-4o using Horizon-Curriculum Learning. Here, the difference between the baseline and Horizon-Curriculum learning was the ordering of the raw trajectories given to the LLM, and the labeling of the trajectories into opening-game, mid-game, and end-game scenarios.*



### BALROG Results

![BALROG results 1](/assets/sarl/exp_1_balrog.png)
*Figure # : Zero-shot game progress (%) in Experiment 1 (Short-Term Memory + Chain-of-Thought) across BALROGenvironments. Bars report the average percentage of task comple- tion achieved by each agent in BabyAI, Crafter,and TextWorld, with error bars indicating variability across evaluation episodes.*


![BALROG results 2](/assets/sarl/exp_2_balrog.png)
*Figure #: Expected zero-shot game progress (%) in Experiment 2 (In-Context Learning + Short-Term Chain-of-Thought Memory) across BALROG environments.*



![BALROG results 3](/assets/sarl/exp_3_balrog.png)
*Figure #: Expected zero-shot game progress (%) in Experiment 3 (Hybrid Memory: Episodic Memory + In-Context Learning + Chain-of-Thought) across BALROG environments*


![BALROG table](/assets/sarl/table_2_balrog.png)
*Table #: Game progress (%) achieved by each agent across three experimental settings. Progress measures the percentage of task completion within each environment, averaged over evaluation episodes. Results are reported per agent and per experiment*



## Discussion

### LMAct discussion

The **ICRL prompting experiment** did not improve performance. As shown in **Figure 1a**, adding a previous-episode summary generally *reduced* average scores across most numbers of demonstrations, with only a minor exception at 28 episodes. Both the baseline and ICRL variants peak around **25 demonstrations**, after which performance rapidly declines. This failure is largely due to the **coarse episode summaries** (only three steps) and the **sparse, uninformative reward signal** in Tic-Tac-Toe, where rewards are only provided at the end of an episode.

The **Reflexion-style heuristic distillation** performed even worse. Replacing raw demonstrations with a single global heuristic led to significantly lower scores than the baseline, as shown in **Figure 1b**. Although performance again peaks around 25 demonstrations, no consistent improvement trend emerges. The distilled heuristic proved **too abstract**, preventing the model from grounding it effectively in the current game state.

The **contrastive heuristic experiment** was only partially completed due to high API costs. Initial results (**Figure 2**) suggest potential, but the experiment was stopped early and is left as future work.

In contrast, **Horizon-Curriculum Learning** produced the most stable results. As shown in **Figure 3**, ordering demonstrations by game phase led to **higher and more consistent performance**, particularly beyond 26 demonstrations. The curriculum helps the model identify its current game phase and select appropriate actions. However, performance still degrades at very large context sizes, likely due to a **needle-in-a-haystack effect** from excessive context.


### BALROG discussion

**Table 2** and **Figures 4–6** summarize zero-shot game progress across three BALROG environments as memory and contextual information are incrementally added at inference time. Across all settings, performance follows a clear trend of **decreasing with environment complexity**: **BabyAI > Crafter > TextWorld**, reflecting increasing planning horizon, stochasticity, and trajectory variance.

### Experiment 1: Short-Term Memory + Chain-of-Thought  
As shown in **Table 2** and **Figure 4**, agents achieve strong performance in **BabyAI**, with GPT-4o reaching **65% progress**, outperforming GPT-4o-mini (42%) and Gemini (48%). This environment benefits from short horizons, low stochasticity, and dense feedback, where short-term memory and stepwise reasoning are effective.  
In contrast, **Crafter** shows much lower progress (GPT-4o: **25%**), and **TextWorld** remains highly challenging (GPT-4o: **25%**, Gemini: **0%**), highlighting the limits of short-term memory in long-horizon, sparse-reward settings.

### Experiment 2: In-Context Learning (Few-Shot Imitation)  
Adding expert demonstrations improves performance in **BabyAI** (**Figure 5**), with GPT-4o increasing from **65% to 72%** and Gemini from **48% to 53%**. These gains suggest that demonstrations provide reusable local structure when task dynamics are stable.  
However, performance **degrades in Crafter** (GPT-4o: **25% → 22%**) and **declines further in TextWorld** (GPT-4o: **25% → 18%**), indicating that few-shot imitation struggles in environments with high variance and combinatorial complexity.

### Experiment 3: Episodic Memory + In-Context Learning  
The hybrid configuration yields the strongest overall results (**Figure 6**), particularly in **BabyAI**, where GPT-4o reaches **74% progress** and Gemini **55%**.  
In **Crafter**, episodic memory does not meaningfully improve performance and slightly degrades results for all models. **TextWorld** remains the most difficult environment, with progress dropping to **10% for GPT-4o** and remaining at **0% for Gemini**, showing that neither episodic recall nor limited reasoning is sufficient under extreme sparsity and stochasticity.

### Overall Takeaway  
These results show that **memory and in-context information are most beneficial in low-variance, predictable environments** like BabyAI. In contrast, for long-horizon and highly stochastic tasks such as Crafter and TextWorld, **naively accumulating context introduces interference rather than improvement**, reinforcing the need for selective and adaptive context mechanisms rather than raw memory scaling.



## Conclusion 


Across both LMAct and BALROG, our results show that **naively adding more context does not improve LLM agent performance** and often introduces interference. In LMAct, previous-episode summaries and abstract heuristics degraded performance, while **structuring demonstrations into a curriculum by game phase** provided more stable behavior as context size increased.

BALROG experiments reveal a similar pattern: **memory and in-context learning help only in low-variance, short-horizon environments** such as BabyAI. In more complex settings like Crafter and TextWorld, additional context leads to saturation and performance decline, indicating that raw memory accumulation is ineffective under stochasticity and sparse rewards.

Overall, these findings suggest that **robust long-horizon agent behavior requires selective and relevance-aware use of demonstrations**, motivating the formulation of demonstration selection as a structured MDP rather than relying on naive context scaling.
